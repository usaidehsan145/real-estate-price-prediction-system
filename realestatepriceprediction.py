# -*- coding: utf-8 -*-
"""dataScience (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B8qBXFPwd4Yy1yJ4jbQfoN7CqMTGdLi2
"""

import pandas as pd
import numpy as np
import os
import hashlib
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor
import joblib

df = pd.read_csv('Entities.csv')

df.head(1)

# Droppinng unnecessary columns

columns_to_drop = ['Unnamed: 0','property_id','location_id','page_url','location','date_added','agency','agent']
df = df.drop(columns = columns_to_drop)

df

df = df[(df['baths'] != 0) & (df['bedrooms'] != 0) & (df['Total_Area'] != 0)]

property_type_datasets = dict(tuple(df.groupby('property_type')))

# Create separate DataFrames for each property type
house_df = property_type_datasets.get('House', pd.DataFrame())
farm_house_df = property_type_datasets.get('Farm House', pd.DataFrame())
flat_df = property_type_datasets.get('Flat', pd.DataFrame())
upper_portion_df = property_type_datasets.get('Upper Portion', pd.DataFrame())
lower_portion_df = property_type_datasets.get('Lower Portion', pd.DataFrame())
room_df = property_type_datasets.get('Room', pd.DataFrame())
penthouse_df = property_type_datasets.get('Penthouse', pd.DataFrame())

def remove_outliers_iqr_sorted(data, column_name):
    # Sort the DataFrame by the specified column
    data = data.sort_values(by=column_name)

    # Calculate the IQR (Interquartile Range)
    Q1 = data[column_name].quantile(0.25)
    Q3 = data[column_name].quantile(0.75)
    IQR = Q3 - Q1

    # Define the upper and lower bounds to identify outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Remove outliers
    data = data[(data[column_name] >= lower_bound) & (data[column_name] <= upper_bound)]

    return data

"""**house_df**

clearing properties with area<bedrooms+baths area
"""

house_df = house_df[(house_df['baths'] != 0) & (house_df['bedrooms'] != 0) & (house_df['Total_Area'] != 0) & (house_df['price'] != 0)]

min_bedroom_area = 70  # sqft
min_bath_area = 25  # sqft

# Filter based on the condition
house_df = house_df[
    house_df['Total_Area'] > (house_df['bedrooms'] * min_bedroom_area + house_df['baths'] * min_bath_area)
]

house_df = house_df[house_df['Total_Area'] <= 32000]

min_area_row = house_df.loc[house_df['Total_Area'].idxmin()]
max_area_row = house_df.loc[house_df['Total_Area'].idxmax()]

print("Row with Minimum Area:")
print(min_area_row)

print("\nRow with Maximum Area:")
print(max_area_row)

"""cleaning flat_df"""

flat_df = flat_df[(flat_df['price'] != 0) & (flat_df['baths'] != 0) & (flat_df['bedrooms'] != 0) & (flat_df['Total_Area'] != 0)]

flat_df = flat_df[flat_df['Total_Area'] <= 1600]

min_bedroom_size = 60
min_bath_size = 25
threshold_area = (flat_df['bedrooms'] + flat_df['baths']) * 100 + 100

flat_df = flat_df[flat_df['Total_Area'] >= threshold_area]

min_area_row = flat_df.loc[flat_df['Total_Area'].idxmin()]
max_area_row = flat_df.loc[flat_df['Total_Area'].idxmax()]

print("Row with Minimum Area:")
print(min_area_row)

print("\nRow with Maximum Area:")
print(max_area_row)

flat_df.head(40)

"""cleaning upper portion"""

upper_portion_df = upper_portion_df[upper_portion_df['purpose'] != 'For Sale']

upper_portion_df = upper_portion_df[
    (upper_portion_df['baths'] != 0) &
    (upper_portion_df['bedrooms'] != 0) &
    (upper_portion_df['Total_Area'] != 0) &
    (upper_portion_df['price'] != 0)

]

upper_portion_df = upper_portion_df[
    upper_portion_df['Total_Area'] > (upper_portion_df['bedrooms'] * min_bedroom_area + upper_portion_df['baths'] * min_bath_area)
]

upper_portion_df = upper_portion_df[upper_portion_df['Total_Area'] <= 32000]

upper_portion_df.head(40)

"""Lower Portion cleaning"""

lower_portion_df = lower_portion_df[lower_portion_df['purpose'] != 'For Sale']

lower_portion_df = lower_portion_df[
    (lower_portion_df['baths'] != 0) &
    (lower_portion_df['bedrooms'] != 0) &
    (lower_portion_df['Total_Area'] != 0) &
    (lower_portion_df['price'] != 0)
]

lower_portion_df = lower_portion_df[
    lower_portion_df['Total_Area'] > (lower_portion_df['bedrooms'] * min_bedroom_area + lower_portion_df['baths'] * min_bath_area)
]

lower_portion_df = lower_portion_df[lower_portion_df['Total_Area'] <= 32000]

lower_portion_df.head(40)

"""penthouse cleaning"""

penthouse_df = penthouse_df[
    (penthouse_df['baths'] != 0) &
    (penthouse_df['bedrooms'] != 0) &
    (penthouse_df['Total_Area'] != 0) &
    (penthouse_df['Total_Area'] <= 12000) &
    (penthouse_df['price'] != 0)
]

min_bedroom_area = 90
min_bath_area = 30

penthouse_df = penthouse_df[
    penthouse_df['Total_Area'] >= (penthouse_df['bedrooms'] * min_bedroom_area + penthouse_df['baths'] * min_bath_area)
]

penthouse_df.head(40)

min_area_row = penthouse_df.loc[penthouse_df['Total_Area'].idxmin()]
max_area_row = penthouse_df.loc[penthouse_df['Total_Area'].idxmax()]

print("Row with Minimum Area:")
print(min_area_row)

print("\nRow with Maximum Area:")
print(max_area_row)

final_df = pd.concat([house_df, upper_portion_df, lower_portion_df, flat_df, penthouse_df], ignore_index=True)

final_df

"""Data pre-processing"""

# Check for null values
null_values = final_df.isnull().sum()

# Display the count of null values for each column
print("Null Values:")
print(null_values)

# Check unique values in the original 'property_type' column
unique_property_types = final_df['property_type'].unique()
print("Unique values in 'property_type' before one-hot encoding:")
print(unique_property_types)

# Drop the 'province_name' column
final_df = final_df.drop(columns=['province_name'])

"""DATA PRE_PROCESSING"""

final_df['property_type'] = final_df['property_type'].astype('category')
final_df = pd.get_dummies(final_df, columns=['property_type'], prefix='property_type')

final_df

# Function to remove outliers using IQR method
def remove_outliers_iqr(df, column_name):
    # Calculate the IQR for the specified column
    q1 = df[column_name].quantile(0.25)
    q3 = df[column_name].quantile(0.75)
    iqr = q3 - q1

    # Define the lower and upper bounds for outliers
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    print(lower_bound,upper_bound)

    # Filter rows within the bounds
    df_no_outliers = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]

    return df_no_outliers

final_df['baths'].plot(kind = 'box')

column_name_to_remove_outliers = 'baths'


final_df = remove_outliers_iqr(final_df, column_name_to_remove_outliers)

final_df['baths'].plot(kind = 'box')

final_df['price'].plot(kind = 'box')

column_name_to_remove_outliers = 'price'

for _ in range(0,7):
    final_df = remove_outliers_iqr(final_df, column_name_to_remove_outliers)

final_df['price'].plot(kind = 'box')

final_df.shape

column_name_to_remove_outliers = 'baths'

# Remove outliers using the defined function
final_df = remove_outliers_iqr(final_df, column_name_to_remove_outliers)

final_df['bedrooms'].plot(kind = 'box')

column_name_to_remove_outliers = 'bedrooms'

# Remove outliers using the defined function
final_df = remove_outliers_iqr(final_df, column_name_to_remove_outliers)

final_df['bedrooms'].plot(kind = 'box')

final_df

# Find the index of the row with the max number of baths
max_baths_index = final_df['baths'].idxmin()

# Print the row with the max number of baths
house_with_max_baths = final_df.loc[max_baths_index]
print("House with Max Baths:")
print(house_with_max_baths)

# # Calculate the combined area of bedrooms and bathrooms based on property type
# final_df['min_bedroom_area'] = np.where(final_df['property_type_Penthouse'] == 1, 190, 150)
# final_df['min_bath_area'] = np.where(final_df['property_type_Penthouse'] == 1, 60, 50)

# combined_area_bedrooms_baths = 8 * (final_df['bedrooms'] * final_df['min_bedroom_area'] + final_df['baths'] * final_df['min_bath_area'])

# # Remove properties where Total_Area is 8 times greater than the combined area of bedrooms and bathrooms
# final_df = final_df[final_df['Total_Area'] <= combined_area_bedrooms_baths]

# # Drop the temporary columns used for calculations
# final_df.drop(['min_bedroom_area', 'min_bath_area'], axis=1, inplace=True)

final_df.head()

final_df.head(1)

final_df['city'] = final_df['city'].astype('category')
final_df = pd.get_dummies(final_df, columns=['city'], prefix='city_')

final_df.head()

final_df['purpose'] = final_df['purpose'].astype('category')
final_df = pd.get_dummies(final_df, columns=['purpose'], prefix='purpose', drop_first=True)

final_df.nunique()

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Assuming 'final_df' is your DataFrame
# Select columns to scale (excluding non-numeric columns, e.g., 'PropertyType', 'city')
columns_to_scale = ['latitude',	'longitude',	'baths',	'bedrooms',	'Total_Area',	'property_type_Flat'	,'property_type_House'	, 'property_type_Lower Portion',	'property_type_Penthouse'	, 'property_type_Upper Portion'	,'city__Faisalabad',	'city__Islamabad'	,'city__Karachi'	,'city__Lahore'	,'city__Rawalpindi'	,'purpose_For Sale']

# Create a MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the selected columns
final_df[columns_to_scale] = scaler.fit_transform(final_df[columns_to_scale])

# Now 'final_df' contains the scaled values in the specified columns

final_df

final_df.to_csv('final_df.csv')

from sklearn.model_selection import train_test_split

# Assuming 'target_column' is the column you're trying to predict (e.g., 'price')
target_column = 'price'

# Separate features (X) and target variable (y)
X = final_df.drop(target_column, axis=1)
y = final_df[target_column]

# Split the dataset into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting sets
print("Training set shape:", X_train.shape, y_train.shape)
print("Test set shape:", X_test.shape, y_test.shape)

import sklearn.linear_model as linMod
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
import sklearn.tree as tree
from sklearn.tree import DecisionTreeRegressor



# print("Multiple Linear Regression")
# linRegress = linMod.LinearRegression()
# linRegress.fit(X_train, y_train)

# print("Linear R2:", r2_score(y_test, linRegress.predict(X_test)))
# print(linRegress.coef_)
# print(linRegress.intercept_)

from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np
import matplotlib.pyplot as plt

# Best hyperparameters

# Create a new XGBoost regressor with the best parameters
best_xgb_regressor = XGBRegressor(
    learning_rate=0.1,
    max_depth=7,
    min_child_weight=6,
    n_estimators=400
)

# Fit the model with your training data (assuming X_train and y_train are defined)
best_xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = best_xgb_regressor.predict(X_test)

# Calculate R2, Mean Squared Error, Mean Absolute Error, and Root Mean Squared Error
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Display the results
print("XGBoost R2:", r2)
print("XGBoost Mean Squared Error:", mse)
print("XGBoost Mean Absolute Error:", mae)
print("XGBoost Root Mean Squared Error:", rmse)

# Create a scatter plot with red for actual and green for predicted
plt.scatter(y_test, y_pred, alpha=0.5, c=['red' if actual > predicted else 'green' for actual, predicted in zip(y_test, y_pred)])

# Add labels and title
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Scatter Plot of Predicted vs. Actual Values (XGBoost)")

# Add a diagonal line for reference (perfect predictions)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='black', linewidth=2)

# Display the plot
plt.show()

import xgboost as xgb
import pickle
from sklearn.preprocessing import MinMaxScaler

# Assuming xg_model and scaler are already defined

# Save the model and normalization parameters
model_data = {'model': best_xgb_regressor, 'scaler': scaler, 'other_info': 'additional data'}
with open('model.pkl', 'wb') as file:
    pickle.dump(model_data, file)